{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Периодизация с опорой на количественные методы.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i04-ufn4A7ri"
      },
      "source": [
        "Загрузка и импорт нужных функций и пакетов, загрузка репозитория и переход в рабочую папку (персональные данные скрыты, названия файлов не настоящие)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTOhvQrAA61y"
      },
      "source": [
        "!pip install fasttext\n",
        "import fasttext, os, zipfile, re, nltk, random\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from scipy import spatial,stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import metrics\n",
        "from scipy.linalg import norm\n",
        "\n",
        "!git config --global user.email \"почта\"\n",
        "!git config --global user.name \"юзернейм\"\n",
        "!git clone ссылка на репозиторий --depth=1\n",
        "\n",
        "folder=zipfile.ZipFile('путь к архиву с текстами', 'r')\n",
        "folder.extractall('/content')\n",
        "\n",
        "folder_with_texts1='путь к папке, куда распаковались тексты'\n",
        "spisok=eval(open('список авторов','r',encoding='utf-8').read())\n",
        "table=[x.strip().split('\\t') for x in open(\"таблица с метоинформацией текстов\",'r',encoding='cp1251').readlines()]\n",
        "model = fasttext.load_model('путь к модели')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KtsdfnyGJZz"
      },
      "source": [
        "Все нужные функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RmOy6vZGOJx"
      },
      "source": [
        "def str_prepr(stri):\n",
        "  return re.sub(' {2,}',' ',re.sub('[^\\w\\s-]', '', re.sub('_','',re.sub('--','―',stri)))).strip()\n",
        "\n",
        "def date_list(dates,fileids):\n",
        "  for d in [fileid[:-4].split('_')[-1] for fileid in fileids]: \n",
        "    if len(d)==4:\n",
        "      dates.append(d)\n",
        "  dates=np.unique(np.array([dates]))\n",
        "  return list(dates)\n",
        "\n",
        "def dates_dict(date_dict,dates,fileids):\n",
        "    for date in dates:\n",
        "      date_list=[]\n",
        "      for fileid in fileids:\n",
        "        date_id=fileid[:-4].split('_')[-1]\n",
        "        if str(date)==date_id:\n",
        "          date_list.append(fileid)  \n",
        "      date_dict[date]=date_list \n",
        "    return date_dict\n",
        "\n",
        "def text_for_learning(text_of_text,date_dict,dates):\n",
        "  for date in dates:\n",
        "    xml_of_date=corpus_text.raw(fileids=[*date_dict[date]])\n",
        "    xmls=[xml+'</html>' for xml in xml_of_date.split('</html>')]\n",
        "    for xml in xmls:\n",
        "      soup = BeautifulSoup(xml, 'lxml')\n",
        "      text_of_text=text_of_text+'\\n\\n'.join([str_prepr(x.get_text()) for x in soup('p') if len(x.contents)!=0 and x.contents[0].name!='noindex']).lower()+'\\n\\n'\n",
        "  final_text=re.sub('\\n{4,}','\\n\\n',text_of_text)\n",
        "  return final_text.split()\n",
        "\n",
        "\n",
        "def get_data(gen):\n",
        "    try:\n",
        "        for elem in gen:\n",
        "            yield elem\n",
        "    except (RuntimeError, StopIteration):\n",
        "        return  \n",
        "\n",
        "def generate_ngrams_content(date_dict,dates,ngrams_content):\n",
        "  for date in dates:\n",
        "    xml_of_date=corpus_text.raw(fileids=[*date_dict[date]])\n",
        "    xmls=[xml+'</html>' for xml in xml_of_date.split('</html>')]\n",
        "    date_ngram_list=[]\n",
        "    for xml in xmls:\n",
        "      soup = BeautifulSoup(xml, 'lxml')\n",
        "      text_of_text='\\n\\n'.join([x.get_text() for x in soup('p') if len(x.contents)!=0 and x.contents[0].name!='noindex']).lower()\n",
        "      text_of_text = re.sub('̀', '', text_of_text)\n",
        "      text_of_text = re.sub('…','...',text_of_text)\n",
        "      text_of_text = re.sub('(\\. +?){4,}|(\\. +?){2}','', text_of_text)\n",
        "      text_of_text = re.sub('--','―',text_of_text)\n",
        "      ngrams_from_text=get_data(ngrams(text_of_text,4))\n",
        "      for gram in ngrams_from_text:\n",
        "        new_gram=''.join([g for g in gram])\n",
        "        date_ngram_list.append(new_gram)   \n",
        "    ngrams_content.append(date_ngram_list)\n",
        "  ngrams_content=np.array(ngrams_content)\n",
        "  return ngrams_content\n",
        "\n",
        "def metric_chunks_content(date_dict,dates,metric_chunks_all):\n",
        "  for date in dates:\n",
        "    xml_of_date=corpus_text.raw(fileids=[*date_dict[date]])\n",
        "    xmls=[xml+'</html>' for xml in xml_of_date.split('</html>')]\n",
        "    date_chunks=[]\n",
        "    for xml in xmls:\n",
        "      soup = BeautifulSoup(xml, 'lxml')\n",
        "      text_of_text='\\n\\n'.join([x.get_text() for x in soup('p') if len(x.contents)!=0 and x.contents[0].name!='noindex']).lower()\n",
        "      for line in text_of_text.split('\\n'):\n",
        "        if len(re.findall(' \\.',line))==0 and len(line)>1: #цепочки нулей-единиц\n",
        "          chain = ''\n",
        "          for char in line:\n",
        "              if char in 'аяоеуюыёиэ':\n",
        "                  chain = chain + '0'\n",
        "              elif char in '֎̀':\n",
        "                  chain = chain[:-1] + '1'\n",
        "          if '1' in chain and '0' in chain:\n",
        "            date_chunks.append(chain)\n",
        "    metric_chunks_all.append(date_chunks)\n",
        "  metric_chunks_all=np.array(metric_chunks_all)\n",
        "  return metric_chunks_all\n",
        "\n",
        "def find_max(year_vectors,date_dict,dates,ngrams_content,metric_content):\n",
        "    k=1\n",
        "    dlina=len(date_dict.keys())\n",
        "    dist_list=[]\n",
        "    raz_names=[]\n",
        "    maxes=[]\n",
        "    while k!=dlina:\n",
        "      print(k)\n",
        "      period1_ngrams=np.concatenate(ngrams_content[0:k])\n",
        "      period2_ngrams=np.concatenate(ngrams_content[k:dlina])\n",
        "      joined_list_ngrams=np.unique(np.concatenate((period1_ngrams,period2_ngrams), axis=0))\n",
        "      frec_first_ngrams=Counter(period1_ngrams) #частотный словарь первого периода\n",
        "      frec_second_ngrams=Counter(period2_ngrams)\n",
        "      k1=len(frec_first_ngrams)*1000 #коэффициент, чтобы привести частоты к ipm\n",
        "      k2=len(frec_second_ngrams)*1000 #то же самое для второго\n",
        "      vector1_ngrams=np.array([frec_first_ngrams[word]/k1 for word in joined_list_ngrams]) #частотный вектор первого корпуса\n",
        "      vector2_ngrams=np.array([frec_second_ngrams[word]/k2 for word in joined_list_ngrams])\n",
        "\n",
        "      period1_metric=np.concatenate(metric_content[0:k])\n",
        "      period2_metric=np.concatenate(metric_content[k:dlina])\n",
        "      joined_list_metric=np.unique(np.concatenate((period1_metric,period2_metric), axis=0))\n",
        "      frec_first_metric=Counter(period1_metric) #частотный словарь первого периода\n",
        "      frec_second_metric=Counter(period2_metric)\n",
        "      k1=len(frec_first_metric)*1000 #коэффициент, чтобы привести частоты к ipm\n",
        "      k2=len(frec_second_metric)*1000 #то же самое для второго\n",
        "      vector1_metric=np.array([frec_first_metric[word]/k1 for word in joined_list_metric][:100])  #частотный вектор первого корпуса\n",
        "      vector2_metric=np.array([frec_second_metric[word]/k2 for word in joined_list_metric][:100])\n",
        "\n",
        "      embedd1=np.mean(np.array([year_vectors[y] for y in dates[0:k]]), axis=0 )\n",
        "      embedd2=np.mean(np.array([year_vectors[y] for y in dates[k:dlina]]), axis=0 )\n",
        "\n",
        "      vector1=stats.zscore(np.concatenate((vector1_metric,vector1_ngrams,embedd1)))\n",
        "      print(np.std(vector1))\n",
        "      vector2=stats.zscore(np.concatenate((vector2_metric,vector2_ngrams,embedd2)))\n",
        "      print(np.std(vector2))\n",
        "\n",
        "      period1_start=dates[0] #число-начало первого \n",
        "      period1_end=dates[k-1] #число-конец первого периода \n",
        "      period2_start=dates[k] #число-начало второго периода\n",
        "      period2_end=dates[dlina-1] #число-конец второго периода\n",
        "      if  period1_start!=period1_end and period2_start!=period2_end: \n",
        "        razbienie_name='('+ period1_start+'_'+period1_end+')'+'___'+'('+period2_start+'_'+period2_end+')'\n",
        "      elif  period1_start!=period1_end and period2_start==period2_end:\n",
        "        razbienie_name='('+ period1_start+'_'+period1_end+')'+'___'+'('+period2_start+')'\n",
        "      else:\n",
        "        razbienie_name='('+ period1_start+')'+'___'+'('+period2_start+'_'+period2_end+')'\n",
        "          #Создаем список разбиений с id файлов получившихся периодов\n",
        "      raz_names.append(razbienie_name)\n",
        "      cos_dist=1 - np.dot(vector1,vector2)/(norm(vector1)*(norm(vector2)))\n",
        "      dist_list.append([razbienie_name,cos_dist])\n",
        "      print(razbienie_name,cos_dist)\n",
        "      k=k+1\n",
        "    for i in range(1,len(dist_list)-1):\n",
        "      if dist_list[i][1]>dist_list[i-1][1] and dist_list[i][1]>dist_list[i+1][1]:\n",
        "        maxes.append([name]+dist_list[i])\n",
        "    return maxes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyFiyChOGSa6"
      },
      "source": [
        "Делим на 2 периода и находим локальные максимумы расстояния"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UfUb6MyIv9U"
      },
      "source": [
        "maxes_for_write=[]\n",
        "for name in spisok:\n",
        "    print(name)\n",
        "    corpus_text = PlaintextCorpusReader(folder_with_texts1+'/'+name, '.*')\n",
        "    fileids=sorted(corpus_text.fileids(), key=lambda fileid: int(fileid[:-4].split('_')[-1][:4]))\n",
        "    dates=date_list([],fileids)\n",
        "    date_dict=dates_dict({},dates,fileids)\n",
        "    year_vectors={}\n",
        "    for year in dates:\n",
        "      year_vectors[year]=np.mean(np.array([model.get_word_vector(w) for w in text_for_learning('',date_dict,[year])]), axis=0 )\n",
        "    metric_content=metric_chunks_content(date_dict,dates,[])\n",
        "    ngrams_content=generate_ngrams_content(date_dict,dates,[])\n",
        "    maxes_for_write=maxes_for_write+find_max(year_vectors,date_dict,dates,ngrams_content,metric_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFHn9JuuI1DK"
      },
      "source": [
        "Проверяем все локальные максимумы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl683hVFKQSL"
      },
      "source": [
        "results=[]\n",
        "for r in maxes_for_write:\n",
        "      row1=r[0]\n",
        "      row2=r[1]\n",
        "\n",
        "      period1_board=int(row1.split('_')[1])\n",
        "      period2_board=int(row2.split('_')[0])\n",
        "\n",
        "      corpus_root = 'путь к корпусу'+'имя нужного автора'\n",
        "      corpus_text = PlaintextCorpusReader(corpus_root, '.*')\n",
        "      fileids=sorted([f for f in corpus_text.fileids()], key=lambda fileid: int(fileid[:-4].split('_')[-1][:4]))\n",
        "      date_dict={}\n",
        "      dates=[]\n",
        "      for d in [fileid[:-4].split('_')[-1] for fileid in fileids]:\n",
        "          if len(d)==4:\n",
        "            dates.append(d)\n",
        "      dates=sorted(list(set(dates)))\n",
        "      period1=[d for d in dates if int(d[:4])<=int(period1_board) and int(d[-4:])<=int(period1_board)]\n",
        "      period2=[d for d in dates if int(d[:4])>=int(period2_board)]\n",
        "\n",
        "      def shuffle_files(per,fileids):\n",
        "              needed_datemarks=[]\n",
        "              needed_fileids=[]\n",
        "              dates1=per\n",
        "              for date in sorted(dates1):\n",
        "                for fileid in fileids:\n",
        "                  date_id=fileid[:-4].split('_')[-1]\n",
        "                  if date_id==str(date):\n",
        "                    needed_datemarks.append(fileid.split('_')[-1][:-4])\n",
        "                    needed_fileids.append(fileid)\n",
        "              random.shuffle(needed_datemarks)\n",
        "              new_fileids=list(zip(needed_fileids,needed_datemarks))\n",
        "              return new_fileids\n",
        "\n",
        "      count=0\n",
        "      for i in range(0,100):\n",
        "            print(i)\n",
        "            shuffeled_fileids1=sorted([f for f in shuffle_files(period1,fileids)+shuffle_files(period2,fileids)], key=lambda fileid: int(fileid[1]))\n",
        "            print(shuffeled_fileids1)\n",
        "            date_dict=dates_dict({},dates,shuffeled_fileids1)\n",
        "            year_vectors={}\n",
        "            for year in dates:\n",
        "              year_vectors[year]=np.mean(np.array([model.get_word_vector(w) for w in text_for_learning('',date_dict,[year])]), axis=0 )\n",
        "            metric_content=metric_chunks_content(date_dict,dates,[])\n",
        "            ngrams_content=generate_ngrams_content(date_dict,dates,[])\n",
        "            m1=find_max(year_vectors,date_dict,dates,ngrams_content,metric_content)\n",
        "\n",
        "            if m1 == '('+row1+')___('+row2+')':\n",
        "              count+=1\n",
        "              print('count',count)\n",
        "              results.append((r,count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xxiZj6rLv3D"
      },
      "source": [
        "Делим на 3 периода и находим локальные максимумы расстояния (приведем только новые функции)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUc2KM-2M0Qk"
      },
      "source": [
        "def new_d(year_vectors,p1,p2,fileids):\n",
        "      date_dict1=dates_dict({},p1,fileids)\n",
        "      date_dict2=dates_dict({},p2,fileids)\n",
        "\n",
        "      period1_ngrams=np.concatenate(generate_ngrams_content(date_dict1,p1,[]))\n",
        "      period2_ngrams=np.concatenate(generate_ngrams_content(date_dict2,p2,[]))\n",
        "      joined_list_ngrams=np.unique(np.concatenate((period1_ngrams,period2_ngrams), axis=0))\n",
        "      frec_first_ngrams=Counter(period1_ngrams) #частотный словарь первого периода\n",
        "      frec_second_ngrams=Counter(period2_ngrams)\n",
        "      k1=len(frec_first_ngrams)*1000000 #коэффициент, чтобы привести частоты к ipm\n",
        "      k2=len(frec_second_ngrams)*1000000 #то же самое для второго\n",
        "      vector1_ngrams=[frec_first_ngrams[word]/k1 for word in joined_list_ngrams]  #частотный вектор первого корпуса\n",
        "      vector2_ngrams=[frec_second_ngrams[word]/k2 for word in joined_list_ngrams]\n",
        "\n",
        "      period1_metric=np.concatenate(metric_chunks_content(date_dict1,p1,[]))\n",
        "      period2_metric=np.concatenate(metric_chunks_content(date_dict2,p2,[]))\n",
        "      joined_list_metric=np.unique(np.concatenate((period1_metric,period2_metric), axis=0))\n",
        "      frec_first_metric=Counter(period1_metric) #частотный словарь первого периода\n",
        "      frec_second_metric=Counter(period2_metric)\n",
        "      k1=len(frec_first_metric)*1000000 #коэффициент, чтобы привести частоты к ipm\n",
        "      k2=len(frec_second_metric)*1000000 #то же самое для второго\n",
        "      vector1_metric=[frec_first_metric[word]/k1 for word in joined_list_metric]  #частотный вектор первого корпуса\n",
        "      vector2_metric=[frec_second_metric[word]/k2 for word in joined_list_metric]\n",
        "\n",
        "      embedd1=np.mean(np.array([year_vectors[y] for y in p1]), axis=0 )\n",
        "      embedd2=np.mean(np.array([year_vectors[y] for y in p2]), axis=0 )\n",
        "\n",
        "      vector1=stats.zscore(np.concatenate((vector1_metric,vector1_ngrams,embedd1)))\n",
        "      vector2=stats.zscore(np.concatenate((vector2_metric,vector2_ngrams,embedd2)))\n",
        "\n",
        "      period1_start=p1[0] #число-начало первого \n",
        "      period1_end=p1[-1] #число-конец первого периода \n",
        "      period2_start=p2[0]\n",
        "      period2_end=p2[-1] #число-конец второго периода\n",
        "      if  period1_start!=period1_end and period2_start!=period2_end: \n",
        "        razbienie_name='('+ period1_start+'_'+period1_end+')'+'___'+'('+period2_start+'_'+period2_end+')'\n",
        "      elif  period1_start!=period1_end and period2_start==period2_end:\n",
        "        razbienie_name='('+ period1_start+'_'+period1_end+')'+'___'+'('+period2_start+')'\n",
        "      else:\n",
        "        razbienie_name='('+ period1_start+')'+'___'+'('+period2_start+'_'+period2_end+')'\n",
        "      return [razbienie_name,spatial.distance.cosine(vector1, vector2)]\n",
        "\n",
        "def three_subs(subs,a):\n",
        "    for i in range(0,len(a)):\n",
        "      for k in range(1,len(a)-1):\n",
        "        for j in range(len(a),1,-1):\n",
        "          period1=a[:i+1]\n",
        "          period2=a[i+1:k+1]\n",
        "          period3=a[k+1:j+1]\n",
        "          if len(period1)+len(period2)+len(period3)==len(a) and len(period1)>0 and len(period2)>0 and len(period3)>0 and [period1,period2,period3] not in subs:\n",
        "            subs.append([period1,period2,period3])\n",
        "    return subs\n",
        "\n",
        "for name in spisok: \n",
        "    print(name)\n",
        "    corpus_root = folder_with_texts1+'/'+name\n",
        "    corpus_text = PlaintextCorpusReader(corpus_root, '.*')\n",
        "    fileids=sorted(corpus_text.fileids(), key=lambda fileid: int(fileid[:-4].split('_')[-1][:4]))\n",
        "    dates=list(date_list([],fileids))\n",
        "    date_dict=dates_dict({},dates,fileids)\n",
        "    year_vectors={}\n",
        "    for year in dates:\n",
        "      year_vectors[year]=np.mean(np.array([model.get_word_vector(w) for w in text_for_learning('',date_dict,[year])]), axis=0 )\n",
        "    subs=list(three_subs([],dates))\n",
        "    print(len(subs))\n",
        "    q=0\n",
        "    d4=[]\n",
        "    for sub in subs:\n",
        "        q=q+1\n",
        "        print(q)\n",
        "        d1=new_d(year_vectors,sub[0],sub[1],fileids)\n",
        "        d2=new_d(year_vectors,sub[1],sub[2],fileids)\n",
        "        dx=d1+d2\n",
        "        d4.append(dx)\n",
        "    for i in range(1,len(d4)-1):\n",
        "      if d4[i][1]>d4[i-1][1] and d4[i][1]>d4[i+1][1] and d4[i][3]>d4[i-1][3] and d4[i][3]>d4[i+1][3] and len(d4[i][0]+d4[i][2])==50:\n",
        "        print('разбиение на три части',d4[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAl0IQFsQJO8"
      },
      "source": [
        "Вот как получаются все варианты разбиения на 3 части:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9sxlGgvrRe9",
        "outputId": "c349e56d-fe5e-4ee1-dec2-839a8ff046da"
      },
      "source": [
        "three_subs([],[1900,1901,1902,1903,1904,1905,1906,1907,1908])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1900], [1901], [1902, 1903, 1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900], [1901, 1902], [1903, 1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900], [1901, 1902, 1903], [1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900], [1901, 1902, 1903, 1904], [1905, 1906, 1907, 1908]],\n",
              " [[1900], [1901, 1902, 1903, 1904, 1905], [1906, 1907, 1908]],\n",
              " [[1900], [1901, 1902, 1903, 1904, 1905, 1906], [1907, 1908]],\n",
              " [[1900], [1901, 1902, 1903, 1904, 1905, 1906, 1907], [1908]],\n",
              " [[1900, 1901], [1902], [1903, 1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901], [1902, 1903], [1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901], [1902, 1903, 1904], [1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901], [1902, 1903, 1904, 1905], [1906, 1907, 1908]],\n",
              " [[1900, 1901], [1902, 1903, 1904, 1905, 1906], [1907, 1908]],\n",
              " [[1900, 1901], [1902, 1903, 1904, 1905, 1906, 1907], [1908]],\n",
              " [[1900, 1901, 1902], [1903], [1904, 1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902], [1903, 1904], [1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902], [1903, 1904, 1905], [1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902], [1903, 1904, 1905, 1906], [1907, 1908]],\n",
              " [[1900, 1901, 1902], [1903, 1904, 1905, 1906, 1907], [1908]],\n",
              " [[1900, 1901, 1902, 1903], [1904], [1905, 1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903], [1904, 1905], [1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903], [1904, 1905, 1906], [1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903], [1904, 1905, 1906, 1907], [1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904], [1905], [1906, 1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904], [1905, 1906], [1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904], [1905, 1906, 1907], [1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904, 1905], [1906], [1907, 1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904, 1905], [1906, 1907], [1908]],\n",
              " [[1900, 1901, 1902, 1903, 1904, 1905, 1906], [1907], [1908]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU5MpaYRNG08"
      },
      "source": [
        "Проверяем варианты разбиения на 3 периода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnsOWIckNi-T"
      },
      "source": [
        "results=[]\n",
        "for r in maxes_for_write:\n",
        "\n",
        "    period1_board=int(r[0].split('_')[1])\n",
        "    period2_board1=int(r[1].split('_')[0])\n",
        "    period2_board2=int(r[1].split('_')[1])\n",
        "    period3_board=int(r[2].split('_')[0])\n",
        "\n",
        "    corpus_root = 'путь к папке'+'имя нужного автора'\n",
        "    corpus_text = PlaintextCorpusReader(corpus_root, '.*')\n",
        "    fileids=sorted([f for f in corpus_text.fileids()], key=lambda fileid: int(fileid[:-4].split('_')[-1][:4]))\n",
        "    date_dict={}\n",
        "    dates=[]\n",
        "    for d in [fileid[:-4].split('_')[-1] for fileid in fileids]:\n",
        "        if len(d)==4:\n",
        "          dates.append(d)\n",
        "    dates=sorted(list(set(dates)))\n",
        "    period1=[d for d in dates if int(d[:4])<=int(period1_board)]\n",
        "    period2=[d for d in dates if int(d[:4])>=int(period2_board1) and int(d[-4:])<=int(period2_board2)]\n",
        "    period3=[d for d in dates if int(d[:4])>=int(period3_board)]\n",
        "\n",
        "    dates1=period1+period2\n",
        "    print(dates1)\n",
        "    dates2=period2+period3\n",
        "    print(dates2)\n",
        "\n",
        "    count=0\n",
        "    for i in range(0,100):\n",
        "      print(i)\n",
        "      shuffeled_fileids1=sorted([f for f in shuffle_files(period1,fileids)+shuffle_files(period2,fileids)], key=lambda fileid: int(fileid[1]))\n",
        "      print(shuffeled_fileids1)\n",
        "      date_dict=dates_dict({},dates1,shuffeled_fileids1)\n",
        "      year_vectors={}\n",
        "      for year in dates1:\n",
        "        year_vectors[year]=np.mean(np.array([model.get_word_vector(w) for w in text_for_learning('',date_dict,[year])]), axis=0 )\n",
        "      metric_content=metric_chunks_content(date_dict,dates1,[])\n",
        "      ngrams_content=generate_ngrams_content(date_dict,dates1,[])\n",
        "      m1=find_max(year_vectors,date_dict,dates1,ngrams_content,metric_content)\n",
        "\n",
        "      shuffeled_fileids2=sorted([f for f in shuffle_files(period2,fileids)+shuffle_files(period3,fileids)], key=lambda fileid: int(fileid[1]))\n",
        "      print(shuffeled_fileids2)\n",
        "      date_dict=dates_dict({},dates2,shuffeled_fileids2)\n",
        "      year_vectors={}\n",
        "      for year in dates2:\n",
        "        year_vectors[year]=np.mean(np.array([model.get_word_vector(w) for w in text_for_learning('',date_dict,[year])]), axis=0 )\n",
        "      metric_content=metric_chunks_content(date_dict,dates2,[])\n",
        "      ngrams_content=generate_ngrams_content(date_dict,dates2,[])\n",
        "      m2=find_max(year_vectors,date_dict,dates2,ngrams_content,metric_content)\n",
        "      if m1 == '('+r[0]+')___('+r[1]+')' and m2 == '('+r[1]+')___('+r[2]+')':\n",
        "        count+=1\n",
        "        print('count',count)\n",
        "      results.append([m1,m2,count])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}